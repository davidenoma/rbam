<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/tests/run_tests.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/run_tests.py" />
              <option name="updatedContent" value="#!/usr/bin/env python3&#10;&quot;&quot;&quot;&#10;Test runner for RBAM project tests&#10;&#10;EXAMPLE EXECUTION COMMANDS:&#10;&#10;1. Run all tests:&#10;   python tests/run_tests.py&#10;&#10;2. Run specific module tests:&#10;   python tests/run_tests.py --module latent&#10;   python tests/run_tests.py --module rbam&#10;&#10;3. Run with coverage reporting:&#10;   python tests/run_tests.py --coverage&#10;&#10;4. Run with verbose output:&#10;   python tests/run_tests.py --verbose&#10;&#10;5. Run and save results to file:&#10;   python tests/run_tests.py &gt; test_results.txt 2&gt;&amp;1&#10;&#10;6. Run in continuous integration mode:&#10;   python tests/run_tests.py --ci&#10;&#10;7. Run with specific test pattern:&#10;   python tests/run_tests.py --pattern &quot;*vae*&quot;&#10;&#10;8. Run with environment controls:&#10;   CUDA_VISIBLE_DEVICES='' python tests/run_tests.py --module rbam&#10;   TF_CPP_MIN_LOG_LEVEL=2 python tests/run_tests.py&#10;&#10;ALTERNATIVE UNITTEST COMMANDS:&#10;&#10;9. Direct unittest execution:&#10;   python -m unittest tests.test_latent_space_predictor -v&#10;   python -m unittest tests.test_rbam_main -v&#10;&#10;10. Run specific test classes:&#10;    python -m unittest tests.test_latent_space_predictor.TestVAE -v&#10;    python -m unittest tests.test_rbam_main.TestRBAMVAE -v&#10;&#10;11. Run specific test methods:&#10;    python -m unittest tests.test_latent_space_predictor.TestVAE.test_vae_initialization -v&#10;    python -m unittest tests.test_rbam_main.TestRBAMVAELoss.test_vae_loss_function -v&#10;&#10;PYTEST COMMANDS (if pytest is installed):&#10;&#10;12. Run with pytest:&#10;    python -m pytest tests/ -v&#10;    python -m pytest tests/test_latent_space_predictor.py -v&#10;    python -m pytest tests/test_rbam_main.py -v&#10;&#10;13. Run with coverage using pytest:&#10;    python -m pytest tests/ --cov=runner --cov-report=html --cov-report=term&#10;&#10;14. Run specific tests with pytest:&#10;    python -m pytest tests/ -k &quot;vae&quot; -v&#10;    python -m pytest tests/ -k &quot;TestVAE&quot; -v&#10;&quot;&quot;&quot;&#10;&#10;import unittest&#10;import sys&#10;import os&#10;import argparse&#10;import time&#10;from io import StringIO&#10;&#10;# Add the project root to the path&#10;project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))&#10;sys.path.insert(0, project_root)&#10;&#10;def print_banner():&#10;    &quot;&quot;&quot;Print a nice banner for the test runner&quot;&quot;&quot;&#10;    print(&quot;=&quot; * 70)&#10;    print(&quot;  RBAM Test Suite Runner&quot;)&#10;    print(&quot;  Representation Learning-Based Association Mapping&quot;)&#10;    print(&quot;=&quot; * 70)&#10;&#10;def setup_environment():&#10;    &quot;&quot;&quot;Setup the testing environment&quot;&quot;&quot;&#10;    # Suppress TensorFlow warnings for cleaner output&#10;    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'&#10;    &#10;    # Set up TensorFlow for testing&#10;    try:&#10;        import tensorflow as tf&#10;        tf.config.run_functions_eagerly(True)&#10;        tf.get_logger().setLevel('ERROR')&#10;        &#10;        # Configure GPU memory growth if GPUs are available&#10;        physical_devices = tf.config.list_physical_devices('GPU')&#10;        if physical_devices:&#10;            for device in physical_devices:&#10;                try:&#10;                    tf.config.experimental.set_memory_growth(device, True)&#10;                except:&#10;                    pass&#10;            print(f&quot; GPU Setup: {len(physical_devices)} GPU(s) available&quot;)&#10;        else:&#10;            print(&quot; Running on CPU (no GPU detected)&quot;)&#10;    except ImportError:&#10;        print(&quot;⚠️  TensorFlow not available&quot;)&#10;&#10;def discover_tests(pattern='test_*.py', start_dir=None):&#10;    &quot;&quot;&quot;Discover all test files&quot;&quot;&quot;&#10;    if start_dir is None:&#10;        start_dir = os.path.dirname(__file__)&#10;    &#10;    loader = unittest.TestLoader()&#10;    suite = loader.discover(start_dir, pattern=pattern)&#10;    return suite&#10;&#10;def run_all_tests(verbosity=2, pattern='test_*.py'):&#10;    &quot;&quot;&quot;Run all tests in the tests directory&quot;&quot;&quot;&#10;    print(f&quot; Discovering tests with pattern: {pattern}&quot;)&#10;    &#10;    suite = discover_tests(pattern=pattern)&#10;    runner = unittest.TextTestRunner(verbosity=verbosity, buffer=True)&#10;    &#10;    start_time = time.time()&#10;    result = runner.run(suite)&#10;    end_time = time.time()&#10;    &#10;    # Print summary&#10;    print(&quot;\n&quot; + &quot;=&quot; * 50)&#10;    print(&quot;TEST SUMMARY&quot;)&#10;    print(&quot;=&quot; * 50)&#10;    print(f&quot;Tests run: {result.testsRun}&quot;)&#10;    print(f&quot;Failures: {len(result.failures)}&quot;)&#10;    print(f&quot;Errors: {len(result.errors)}&quot;)&#10;    print(f&quot;Skipped: {len(result.skipped) if hasattr(result, 'skipped') else 0}&quot;)&#10;    print(f&quot;Success rate: {((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100):.1f}%&quot; if result.testsRun &gt; 0 else &quot;N/A&quot;)&#10;    print(f&quot;Total time: {end_time - start_time:.2f} seconds&quot;)&#10;    &#10;    # Print failures and errors if any&#10;    if result.failures:&#10;        print(f&quot;\n❌ FAILURES ({len(result.failures)}):&quot;)&#10;        for test, traceback in result.failures:&#10;            print(f&quot;  - {test}&quot;)&#10;    &#10;    if result.errors:&#10;        print(f&quot;\n ERRORS ({len(result.errors)}):&quot;)&#10;        for test, traceback in result.errors:&#10;            print(f&quot;  - {test}&quot;)&#10;    &#10;    return result.wasSuccessful()&#10;&#10;def run_specific_test(test_module, verbosity=2):&#10;    &quot;&quot;&quot;Run tests for a specific module&quot;&quot;&quot;&#10;    module_map = {&#10;        'latent': 'test_latent_space_predictor',&#10;        'rbam': 'test_rbam_main'&#10;    }&#10;    &#10;    if test_module not in module_map:&#10;        print(f&quot;❌ Unknown test module: {test_module}&quot;)&#10;        print(f&quot;Available modules: {', '.join(module_map.keys())}&quot;)&#10;        return False&#10;    &#10;    module_name = module_map[test_module]&#10;    print(f&quot; Running {module_name} tests...&quot;)&#10;    &#10;    try:&#10;        # Import the test module&#10;        test_module_obj = __import__(f'tests.{module_name}', fromlist=[''])&#10;        &#10;        # Load tests from the module&#10;        loader = unittest.TestLoader()&#10;        suite = loader.loadTestsFromModule(test_module_obj)&#10;        &#10;        # Run the tests&#10;        runner = unittest.TextTestRunner(verbosity=verbosity, buffer=True)&#10;        start_time = time.time()&#10;        result = runner.run(suite)&#10;        end_time = time.time()&#10;        &#10;        print(f&quot;\n⏱️  Completed in {end_time - start_time:.2f} seconds&quot;)&#10;        return result.wasSuccessful()&#10;        &#10;    except ImportError as e:&#10;        print(f&quot;❌ Failed to import {module_name}: {e}&quot;)&#10;        return False&#10;&#10;def run_with_coverage():&#10;    &quot;&quot;&quot;Run tests with coverage reporting&quot;&quot;&quot;&#10;    try:&#10;        import coverage&#10;        print(&quot; Running tests with coverage analysis...&quot;)&#10;        &#10;        # Start coverage&#10;        cov = coverage.Coverage()&#10;        cov.start()&#10;        &#10;        # Run tests&#10;        success = run_all_tests(verbosity=1)&#10;        &#10;        # Stop coverage and save&#10;        cov.stop()&#10;        cov.save()&#10;        &#10;        # Generate reports&#10;        print(&quot;\n Coverage Report:&quot;)&#10;        print(&quot;-&quot; * 50)&#10;        cov.report()&#10;        &#10;        # Generate HTML report&#10;        html_dir = os.path.join(project_root, 'htmlcov')&#10;        cov.html_report(directory=html_dir)&#10;        print(f&quot;\n HTML coverage report generated: {html_dir}/index.html&quot;)&#10;        &#10;        return success&#10;        &#10;    except ImportError:&#10;        print(&quot;❌ Coverage.py not installed. Install with: pip install coverage&quot;)&#10;        print(&quot; Running tests without coverage...&quot;)&#10;        return run_all_tests()&#10;&#10;def main():&#10;    &quot;&quot;&quot;Main function to run tests based on command line arguments&quot;&quot;&quot;&#10;    parser = argparse.ArgumentParser(&#10;        description='Run RBAM tests',&#10;        formatter_class=argparse.RawDescriptionHelpFormatter,&#10;        epilog=&quot;&quot;&quot;&#10;Examples:&#10;  python tests/run_tests.py                    # Run all tests&#10;  python tests/run_tests.py --module latent    # Run latent space tests only&#10;  python tests/run_tests.py --coverage         # Run with coverage&#10;  python tests/run_tests.py --verbose          # Run with verbose output&#10;  python tests/run_tests.py --ci               # Run in CI mode&#10;        &quot;&quot;&quot;&#10;    )&#10;    &#10;    parser.add_argument('--module', choices=['latent', 'rbam', 'all'], default='all',&#10;                       help='Which module to test (default: all)')&#10;    parser.add_argument('--coverage', action='store_true',&#10;                       help='Run with coverage reporting (requires coverage.py)')&#10;    parser.add_argument('--verbose', '-v', action='store_true',&#10;                       help='Verbose output')&#10;    parser.add_argument('--ci', action='store_true',&#10;                       help='Continuous integration mode (less verbose, structured output)')&#10;    parser.add_argument('--pattern', default='test_*.py',&#10;                       help='Test file pattern (default: test_*.py)')&#10;    parser.add_argument('--quiet', '-q', action='store_true',&#10;                       help='Quiet mode (minimal output)')&#10;    &#10;    args = parser.parse_args()&#10;    &#10;    # Set verbosity level&#10;    if args.quiet:&#10;        verbosity = 0&#10;    elif args.verbose:&#10;        verbosity = 2&#10;    elif args.ci:&#10;        verbosity = 1&#10;    else:&#10;        verbosity = 2&#10;    &#10;    # Setup&#10;    if not args.quiet:&#10;        print_banner()&#10;    &#10;    setup_environment()&#10;    &#10;    # Run tests&#10;    success = False&#10;    &#10;    try:&#10;        if args.coverage:&#10;            success = run_with_coverage()&#10;        elif args.module == 'all':&#10;            if not args.quiet:&#10;                print(&quot; Running all tests...&quot;)&#10;            success = run_all_tests(verbosity=verbosity, pattern=args.pattern)&#10;        else:&#10;            success = run_specific_test(args.module, verbosity=verbosity)&#10;            &#10;        # Final status&#10;        if not args.quiet:&#10;            print(&quot;\n&quot; + &quot;=&quot; * 70)&#10;            if success:&#10;                print(&quot;✅ ALL TESTS PASSED!&quot;)&#10;            else:&#10;                print(&quot;❌ SOME TESTS FAILED!&quot;)&#10;            print(&quot;=&quot; * 70)&#10;            &#10;    except KeyboardInterrupt:&#10;        print(&quot;\n⚠️  Tests interrupted by user&quot;)&#10;        success = False&#10;    except Exception as e:&#10;        print(f&quot;\n Unexpected error: {e}&quot;)&#10;        success = False&#10;    &#10;    # Exit with appropriate code&#10;    sys.exit(0 if success else 1)&#10;&#10;if __name__ == '__main__':&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>